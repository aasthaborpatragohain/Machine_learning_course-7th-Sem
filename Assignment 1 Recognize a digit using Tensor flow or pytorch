# STEP 1: Import libraries
import tensorflow as tf                       # TensorFlow: main deep-learning library (models, layers, training loops)
from tensorflow.keras.datasets import mnist   # MNIST loader: gives train/test images and labels
import numpy as np                            # NumPy: fast numeric arrays and utilities
import matplotlib.pyplot as plt               # Matplotlib's pyplot: plotting functions (imshow, show, title, etc.)

# STEP 2: Load MNIST dataset
(x_train, y_train), (x_test, y_test) = mnist.load_data()
# mnist.load_data() returns:
#   x_train: array of shape (60000, 28, 28), dtype uint8, pixel values 0..255
#   y_train: array of shape (60000,), integer labels 0..9
#   x_test, y_test: analogous for test set (10000 samples)

# STEP 3: Normalize pixel values (0..255 -> 0.0..1.0) and ensure efficient dtype
x_train = x_train.astype('float32') / 255.0  # cast to float32 (better for TF) then scale to [0,1]
x_test  = x_test.astype('float32')  / 255.0  # do the same for test images
# Normalization helps training converge faster and avoids very large gradients.

# (optional) ensure labels are integer dtype (they already are), explicit cast for clarity:
y_train = y_train.astype('int64')
y_test  = y_test.astype('int64')

# STEP 4: Build a simple Neural Network using the Sequential API
model = tf.keras.models.Sequential([         # Sequential runs layers in the given order
    tf.keras.layers.Flatten(input_shape=(28, 28)),  # reshape each image from (28,28) -> (784,)
    # Flatten does not change batch size; it transforms each sample independently.

    tf.keras.layers.Dense(128, activation='relu'),  # Dense = fully connected layer with 128 neurons
    # Each neuron computes a weighted sum + bias, then ReLU(x)=max(0,x) activation is applied.

    tf.keras.layers.Dense(10, activation='softmax') # Output layer: 10 neurons (one per digit 0-9)
    # Softmax converts the 10 outputs into probabilities that sum to 1.
])

# STEP 5: Compile the model: choose optimizer, loss, and metrics
model.compile(
    optimizer='adam',                             # Adam optimizer: popular, adaptive learning rates
    loss='sparse_categorical_crossentropy',       # Loss for integer labels (0..9). Don't need one-hot vectors.
    metrics=['accuracy']                          # Track accuracy metric during training & evaluation
)

# STEP 6: Train the model on the training dataset
history = model.fit(
    x_train, y_train,
    epochs=5,            # number of passes over the entire training set
    batch_size=32,       # optional explicit batch size (TF default is 32)
    verbose=1            # prints training progress per epoch (use 0 to silence)
)
# model.fit returns a History object with epoch-wise loss & metrics at history.history

# STEP 7: Evaluate model performance on the test dataset
test_loss, test_accuracy = model.evaluate(
    x_test, y_test,
    verbose=1            # show evaluation progress
)
print(f"\n Test Accuracy: {test_accuracy:.4f}")  # formatted print: 4 decimal places

# STEP 8: Predict and display the first 10 test images
num_images = 10         # how many images to visualize/predict
correct = 0             # counter for correct predictions

for i in range(num_images):
    img = x_test[i]                 # i-th test image, shape (28, 28), values in [0.0, 1.0]
    actual = int(y_test[i])         # true label (convert to plain Python int for readability)

    # model.predict expects a batch dimension: shape (batch_size, 28, 28)
    # we create a batch of one image using np.expand_dims
    batch = np.expand_dims(img, axis=0)    # shape -> (1, 28, 28)

    # get model outputs (probabilities) for the batch; verbose=0 avoids per-call logging
    prediction = model.predict(batch, verbose=0)  # shape (1, 10): probabilities for each class

    # choose the class with highest probability
    # np.argmax(prediction, axis=1) gives an array of length batch_size; [0] picks the first (and only) element
    predicted = int(np.argmax(prediction, axis=1)[0])

    # display the image with a colored title: green if correct, red if incorrect
    plt.figure(figsize=(2, 2))                # small figure for each image
    plt.imshow(img, cmap='gray', interpolation='nearest')  # show grayscale image
    plt.title(f"Actual: {actual} | Predicted: {predicted}",
              color='green' if predicted == actual else 'red')
    plt.axis('off')                           # hide axis ticks & labels
    plt.show()                                # render the figure (important in notebooks/Colab)

    # count correct predictions
    if predicted == actual:
        correct += 1

# final summary of how many of the first num_images were predicted correctly
print(f"\n Correct Predictions: {correct}/{num_images}")
